{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Faster R-CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvDK1kl/jxYDUAoDvFM4JC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":74,"metadata":{"id":"KUvhUfDYznzX","executionInfo":{"status":"ok","timestamp":1659554531231,"user_tz":-180,"elapsed":5,"user":{"displayName":"Özgür Çetin","userId":"15568653990884069427"}}},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision.transforms as T\n","import torchvision\n","import torch\n","import numpy as np\n","import cv2\n","import os"]},{"cell_type":"code","source":["COCO_INSTANCE_CATEGORY_NAMES = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n","    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n","    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n","    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n","    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n","    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n","]"],"metadata":{"id":"aZZ24vQE1S7Z","executionInfo":{"status":"ok","timestamp":1659554531232,"user_tz":-180,"elapsed":5,"user":{"displayName":"Özgür Çetin","userId":"15568653990884069427"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True).eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ZKklaw61S5U","executionInfo":{"status":"ok","timestamp":1659554532114,"user_tz":-180,"elapsed":887,"user":{"displayName":"Özgür Çetin","userId":"15568653990884069427"}},"outputId":"f0c9396a-61e6-44dc-bdd4-7630f3073a97"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["def get_prediction(img_path, threshold):\n","  \"\"\"\n","  get_prediction\n","    parameters:\n","      - img_path - path of the input image\n","      - threshold - threshold value for prediction score\n","    method:\n","      - Image is obtained from the image path\n","      - the image is converted to image tensor using PyTorch's Transforms\n","      - image is passed through the model to get the predictions\n","      - class, box coordinates are obtained, but only prediction score > threshold\n","        are chosen.\n","    \n","  \"\"\"\n","  img = Image.open(img_path)\n","  transform = T.Compose([T.ToTensor()])\n","  img = transform(img)\n","  pred = model([img])\n","  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n","  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n","  pred_score = list(pred[0]['scores'].detach().numpy())\n","  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n","  pred_boxes = pred_boxes[:pred_t+1]\n","  pred_class = pred_class[:pred_t+1]\n","  return pred_boxes, pred_class"],"metadata":{"id":"tRx-1h6U1S2s","executionInfo":{"status":"ok","timestamp":1659554536254,"user_tz":-180,"elapsed":316,"user":{"displayName":"Özgür Çetin","userId":"15568653990884069427"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n","  \"\"\"\n","  object_detection_api\n","    parameters:\n","      - img_path - path of the input image\n","      - threshold - threshold value for prediction score\n","      - rect_th - thickness of bounding box\n","      - text_size - size of the class label text\n","      - text_th - thichness of the text\n","    method:\n","      - prediction is obtained from get_prediction method\n","      - for each prediction, bounding box is drawn and text is written \n","        with opencv\n","      - the final image is displayed\n","  \"\"\"\n","  boxes, pred_cls = get_prediction(img_path, threshold)\n","  img = cv2.imread(img_path)\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  for i in range(len(boxes)):\n","    cv2.rectangle(img, boxes[i][0], boxes[i][1],(1, 255, 1),rect_th)\n","    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n","  plt.figure(figsize=(20,30))\n","  plt.imshow(img)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.show()"],"metadata":{"id":"9qOxOWn8zqsc","executionInfo":{"status":"ok","timestamp":1659554537731,"user_tz":-180,"elapsed":2,"user":{"displayName":"Özgür Çetin","userId":"15568653990884069427"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["!wget https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg -O people.jpg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KN4oWXKPzqqY","executionInfo":{"status":"ok","timestamp":1659554540305,"user_tz":-180,"elapsed":461,"user":{"displayName":"Özgür Çetin","userId":"15568653990884069427"}},"outputId":"bf223151-0cdb-4095-ca36-660be2361ffc"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-08-03 19:22:19--  https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg\n","Resolving www.wsha.org (www.wsha.org)... 141.193.213.10, 141.193.213.11\n","Connecting to www.wsha.org (www.wsha.org)|141.193.213.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1923610 (1.8M) [image/jpeg]\n","Saving to: ‘people.jpg’\n","\n","people.jpg          100%[===================>]   1.83M  --.-KB/s    in 0.1s    \n","\n","2022-08-03 19:22:19 (15.3 MB/s) - ‘people.jpg’ saved [1923610/1923610]\n","\n"]}]},{"cell_type":"code","source":["object_detection_api('/content/people.jpg', threshold=0.8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"_4b7qq0rzqog","executionInfo":{"status":"error","timestamp":1659554569599,"user_tz":-180,"elapsed":7380,"user":{"displayName":"Özgür Çetin","userId":"15568653990884069427"}},"outputId":"29021370-3813-4623-91ee-62d807f95e59"},"execution_count":81,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-790c809d5620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobject_detection_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/people.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-78-a5d181185a76>\u001b[0m in \u001b[0;36mobject_detection_api\u001b[0;34m(img_path, threshold, rect_th, text_size, text_th)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrect_th\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_cls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthickness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_th\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'rectangle'\n> Overload resolution failed:\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'rec'. Expected sequence length 4, got 2\n>  - Can't parse 'rec'. Expected sequence length 4, got 2\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"s338PRaozqmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BQLRxY6RzqkS"},"execution_count":null,"outputs":[]}]}